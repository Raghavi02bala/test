{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference https://machinelearningmastery.com/how-to-develop-a-cnn-from-scratch-for-cifar-10-photo-classification/"
      ],
      "metadata": {
        "id": "wM8g0-7xEIyV"
      }
    },
    {
      "metadata": {
        "id": "cl9S9N3BkPAY"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras import regularizers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m5ugXBDPkbmv"
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 250\n",
        "l = 12\n",
        "num_filter = 36 \n",
        "compression = 0.5 \n",
        "dropout_rate = 0.2"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sxaXneiJkd9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496b3802-f2e2-44dd-bdc9-580921adcbde"
      },
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = X_train.shape[1],X_train.shape[2],X_train.shape[3]\n",
        "\n",
        "y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "GesfG5EPkjEh"
      },
      "cell_type": "code",
      "source": [
        "def denseblock(input, num_filter = 12, dropout_rate = 0.2):\n",
        "  global compression\n",
        "  temp = input\n",
        "  for _ in range(l):\n",
        "      BatchNorm = BatchNormalization()(temp)\n",
        "      relu = Activation('relu')(BatchNorm)\n",
        "      Conv2D_3_3 = Conv2D(int(num_filter*compression), (3,3), use_bias=False ,padding='same')(relu)\n",
        "      concat = Concatenate(axis=-1)([temp,Conv2D_3_3])        \n",
        "      temp = concat        \n",
        "  return temp\n",
        "\n",
        "def transition(input, num_filter = 12, dropout_rate = 0.2):\n",
        "  global compression\n",
        "  BatchNorm = BatchNormalization()(input)\n",
        "  relu = Activation('relu')(BatchNorm)\n",
        "  Conv2D_BottleNeck = Conv2D(int(num_filter*compression), (1,1), use_bias=False, kernel_regularizer = regularizers.l1() ,padding='same')(relu)\n",
        "  avg = AveragePooling2D(pool_size=(2,2))(Conv2D_BottleNeck)  \n",
        "  return avg\n",
        "\n",
        "def output_layer(input):\n",
        "  global compression\n",
        "  BatchNorm = BatchNormalization()(input)\n",
        "  relu = Activation('relu')(BatchNorm)\n",
        "  AvgPooling = AveragePooling2D(pool_size=(2,2))(relu)\n",
        "  temp = Conv2D(num_classes, kernel_size = (2,2))(AvgPooling)\n",
        "  output = Activation('softmax')(temp)\n",
        "  flat = Flatten()(output)\n",
        "  return flat"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aTtwwMKFk3Vf"
      },
      "cell_type": "code",
      "source": [
        "num_filter = 36\n",
        "dropout_rate = 0.2\n",
        "l= 12\n",
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "First_Conv2D = Conv2D(num_filter, (3,3), use_bias=False ,padding='same')(input)\n",
        "\n",
        "First_Block = denseblock(First_Conv2D, num_filter, dropout_rate)\n",
        "First_Transition = transition(First_Block, num_filter, dropout_rate)\n",
        "\n",
        "Second_Block = denseblock(First_Transition, num_filter, dropout_rate)\n",
        "Second_Transition = transition(Second_Block, num_filter, dropout_rate)\n",
        "\n",
        "Third_Block = denseblock(Second_Transition, num_filter, dropout_rate)\n",
        "Third_Transition = transition(Third_Block, num_filter, dropout_rate)\n",
        "\n",
        "Last_Block = denseblock(Third_Transition,  num_filter, dropout_rate)\n",
        "output = output_layer(Last_Block)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84TNf3KTk-dZ",
        "outputId": "154cc286-7efc-41c5-fe47-9d6c4e3bdaa5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 36)   972         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 36)  144         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 36)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 32, 32, 18)   5832        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 32, 32, 54)   0           ['conv2d[0][0]',                 \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 54)  216         ['concatenate[0][0]']            \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 54)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 32, 32, 18)   8748        ['activation_1[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 32, 32, 72)   0           ['concatenate[0][0]',            \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 72)  288         ['concatenate_1[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 72)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 32, 32, 18)   11664       ['activation_2[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 32, 32, 90)   0           ['concatenate_1[0][0]',          \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 32, 32, 90)  360         ['concatenate_2[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 32, 32, 90)   0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 32, 32, 18)   14580       ['activation_3[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 32, 32, 108)  0           ['concatenate_2[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 32, 32, 108)  432        ['concatenate_3[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 32, 32, 108)  0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 32, 32, 18)   17496       ['activation_4[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 32, 32, 126)  0           ['concatenate_3[0][0]',          \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 32, 32, 126)  504        ['concatenate_4[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 32, 32, 126)  0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 18)   20412       ['activation_5[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_5 (Concatenate)    (None, 32, 32, 144)  0           ['concatenate_4[0][0]',          \n",
            "                                                                  'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 32, 32, 144)  576        ['concatenate_5[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 32, 32, 144)  0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 32, 32, 18)   23328       ['activation_6[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_6 (Concatenate)    (None, 32, 32, 162)  0           ['concatenate_5[0][0]',          \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 32, 32, 162)  648        ['concatenate_6[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 32, 32, 162)  0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 32, 32, 18)   26244       ['activation_7[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_7 (Concatenate)    (None, 32, 32, 180)  0           ['concatenate_6[0][0]',          \n",
            "                                                                  'conv2d_8[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 32, 32, 180)  720        ['concatenate_7[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 32, 32, 180)  0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_9 (Conv2D)              (None, 32, 32, 18)   29160       ['activation_8[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_8 (Concatenate)    (None, 32, 32, 198)  0           ['concatenate_7[0][0]',          \n",
            "                                                                  'conv2d_9[0][0]']               \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 32, 32, 198)  792        ['concatenate_8[0][0]']          \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 32, 32, 198)  0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_10 (Conv2D)             (None, 32, 32, 18)   32076       ['activation_9[0][0]']           \n",
            "                                                                                                  \n",
            " concatenate_9 (Concatenate)    (None, 32, 32, 216)  0           ['concatenate_8[0][0]',          \n",
            "                                                                  'conv2d_10[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 32, 32, 216)  864        ['concatenate_9[0][0]']          \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 32, 32, 216)  0           ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_11 (Conv2D)             (None, 32, 32, 18)   34992       ['activation_10[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenate)   (None, 32, 32, 234)  0           ['concatenate_9[0][0]',          \n",
            "                                                                  'conv2d_11[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 32, 32, 234)  936        ['concatenate_10[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 32, 32, 234)  0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_12 (Conv2D)             (None, 32, 32, 18)   37908       ['activation_11[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenate)   (None, 32, 32, 252)  0           ['concatenate_10[0][0]',         \n",
            "                                                                  'conv2d_12[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 32, 32, 252)  1008       ['concatenate_11[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 32, 32, 252)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_13 (Conv2D)             (None, 32, 32, 18)   4536        ['activation_12[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d (AveragePool  (None, 16, 16, 18)  0           ['conv2d_13[0][0]']              \n",
            " ing2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 16, 18)  72          ['average_pooling2d[0][0]']      \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 16, 16, 18)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_14 (Conv2D)             (None, 16, 16, 18)   2916        ['activation_13[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_12 (Concatenate)   (None, 16, 16, 36)   0           ['average_pooling2d[0][0]',      \n",
            "                                                                  'conv2d_14[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 16, 36)  144         ['concatenate_12[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 16, 16, 36)   0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_15 (Conv2D)             (None, 16, 16, 18)   5832        ['activation_14[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenate)   (None, 16, 16, 54)   0           ['concatenate_12[0][0]',         \n",
            "                                                                  'conv2d_15[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 16, 54)  216         ['concatenate_13[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_15 (Activation)     (None, 16, 16, 54)   0           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_16 (Conv2D)             (None, 16, 16, 18)   8748        ['activation_15[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_14 (Concatenate)   (None, 16, 16, 72)   0           ['concatenate_13[0][0]',         \n",
            "                                                                  'conv2d_16[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_16 (BatchN  (None, 16, 16, 72)  288         ['concatenate_14[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_16 (Activation)     (None, 16, 16, 72)   0           ['batch_normalization_16[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_17 (Conv2D)             (None, 16, 16, 18)   11664       ['activation_16[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_15 (Concatenate)   (None, 16, 16, 90)   0           ['concatenate_14[0][0]',         \n",
            "                                                                  'conv2d_17[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_17 (BatchN  (None, 16, 16, 90)  360         ['concatenate_15[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_17 (Activation)     (None, 16, 16, 90)   0           ['batch_normalization_17[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_18 (Conv2D)             (None, 16, 16, 18)   14580       ['activation_17[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_16 (Concatenate)   (None, 16, 16, 108)  0           ['concatenate_15[0][0]',         \n",
            "                                                                  'conv2d_18[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_18 (BatchN  (None, 16, 16, 108)  432        ['concatenate_16[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_18 (Activation)     (None, 16, 16, 108)  0           ['batch_normalization_18[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_19 (Conv2D)             (None, 16, 16, 18)   17496       ['activation_18[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_17 (Concatenate)   (None, 16, 16, 126)  0           ['concatenate_16[0][0]',         \n",
            "                                                                  'conv2d_19[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_19 (BatchN  (None, 16, 16, 126)  504        ['concatenate_17[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_19 (Activation)     (None, 16, 16, 126)  0           ['batch_normalization_19[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_20 (Conv2D)             (None, 16, 16, 18)   20412       ['activation_19[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_18 (Concatenate)   (None, 16, 16, 144)  0           ['concatenate_17[0][0]',         \n",
            "                                                                  'conv2d_20[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_20 (BatchN  (None, 16, 16, 144)  576        ['concatenate_18[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_20 (Activation)     (None, 16, 16, 144)  0           ['batch_normalization_20[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_21 (Conv2D)             (None, 16, 16, 18)   23328       ['activation_20[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_19 (Concatenate)   (None, 16, 16, 162)  0           ['concatenate_18[0][0]',         \n",
            "                                                                  'conv2d_21[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_21 (BatchN  (None, 16, 16, 162)  648        ['concatenate_19[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_21 (Activation)     (None, 16, 16, 162)  0           ['batch_normalization_21[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_22 (Conv2D)             (None, 16, 16, 18)   26244       ['activation_21[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_20 (Concatenate)   (None, 16, 16, 180)  0           ['concatenate_19[0][0]',         \n",
            "                                                                  'conv2d_22[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_22 (BatchN  (None, 16, 16, 180)  720        ['concatenate_20[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_22 (Activation)     (None, 16, 16, 180)  0           ['batch_normalization_22[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_23 (Conv2D)             (None, 16, 16, 18)   29160       ['activation_22[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_21 (Concatenate)   (None, 16, 16, 198)  0           ['concatenate_20[0][0]',         \n",
            "                                                                  'conv2d_23[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_23 (BatchN  (None, 16, 16, 198)  792        ['concatenate_21[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_23 (Activation)     (None, 16, 16, 198)  0           ['batch_normalization_23[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_24 (Conv2D)             (None, 16, 16, 18)   32076       ['activation_23[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_22 (Concatenate)   (None, 16, 16, 216)  0           ['concatenate_21[0][0]',         \n",
            "                                                                  'conv2d_24[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_24 (BatchN  (None, 16, 16, 216)  864        ['concatenate_22[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_24 (Activation)     (None, 16, 16, 216)  0           ['batch_normalization_24[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_25 (Conv2D)             (None, 16, 16, 18)   34992       ['activation_24[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_23 (Concatenate)   (None, 16, 16, 234)  0           ['concatenate_22[0][0]',         \n",
            "                                                                  'conv2d_25[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_25 (BatchN  (None, 16, 16, 234)  936        ['concatenate_23[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_25 (Activation)     (None, 16, 16, 234)  0           ['batch_normalization_25[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_26 (Conv2D)             (None, 16, 16, 18)   4212        ['activation_25[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_1 (AveragePo  (None, 8, 8, 18)    0           ['conv2d_26[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_26 (BatchN  (None, 8, 8, 18)    72          ['average_pooling2d_1[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_26 (Activation)     (None, 8, 8, 18)     0           ['batch_normalization_26[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_27 (Conv2D)             (None, 8, 8, 18)     2916        ['activation_26[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_24 (Concatenate)   (None, 8, 8, 36)     0           ['average_pooling2d_1[0][0]',    \n",
            "                                                                  'conv2d_27[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_27 (BatchN  (None, 8, 8, 36)    144         ['concatenate_24[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_27 (Activation)     (None, 8, 8, 36)     0           ['batch_normalization_27[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_28 (Conv2D)             (None, 8, 8, 18)     5832        ['activation_27[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_25 (Concatenate)   (None, 8, 8, 54)     0           ['concatenate_24[0][0]',         \n",
            "                                                                  'conv2d_28[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_28 (BatchN  (None, 8, 8, 54)    216         ['concatenate_25[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_28 (Activation)     (None, 8, 8, 54)     0           ['batch_normalization_28[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_29 (Conv2D)             (None, 8, 8, 18)     8748        ['activation_28[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_26 (Concatenate)   (None, 8, 8, 72)     0           ['concatenate_25[0][0]',         \n",
            "                                                                  'conv2d_29[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_29 (BatchN  (None, 8, 8, 72)    288         ['concatenate_26[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_29 (Activation)     (None, 8, 8, 72)     0           ['batch_normalization_29[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_30 (Conv2D)             (None, 8, 8, 18)     11664       ['activation_29[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_27 (Concatenate)   (None, 8, 8, 90)     0           ['concatenate_26[0][0]',         \n",
            "                                                                  'conv2d_30[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_30 (BatchN  (None, 8, 8, 90)    360         ['concatenate_27[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_30 (Activation)     (None, 8, 8, 90)     0           ['batch_normalization_30[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_31 (Conv2D)             (None, 8, 8, 18)     14580       ['activation_30[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_28 (Concatenate)   (None, 8, 8, 108)    0           ['concatenate_27[0][0]',         \n",
            "                                                                  'conv2d_31[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_31 (BatchN  (None, 8, 8, 108)   432         ['concatenate_28[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_31 (Activation)     (None, 8, 8, 108)    0           ['batch_normalization_31[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_32 (Conv2D)             (None, 8, 8, 18)     17496       ['activation_31[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_29 (Concatenate)   (None, 8, 8, 126)    0           ['concatenate_28[0][0]',         \n",
            "                                                                  'conv2d_32[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_32 (BatchN  (None, 8, 8, 126)   504         ['concatenate_29[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_32 (Activation)     (None, 8, 8, 126)    0           ['batch_normalization_32[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_33 (Conv2D)             (None, 8, 8, 18)     20412       ['activation_32[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_30 (Concatenate)   (None, 8, 8, 144)    0           ['concatenate_29[0][0]',         \n",
            "                                                                  'conv2d_33[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_33 (BatchN  (None, 8, 8, 144)   576         ['concatenate_30[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_33 (Activation)     (None, 8, 8, 144)    0           ['batch_normalization_33[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_34 (Conv2D)             (None, 8, 8, 18)     23328       ['activation_33[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_31 (Concatenate)   (None, 8, 8, 162)    0           ['concatenate_30[0][0]',         \n",
            "                                                                  'conv2d_34[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_34 (BatchN  (None, 8, 8, 162)   648         ['concatenate_31[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_34 (Activation)     (None, 8, 8, 162)    0           ['batch_normalization_34[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_35 (Conv2D)             (None, 8, 8, 18)     26244       ['activation_34[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_32 (Concatenate)   (None, 8, 8, 180)    0           ['concatenate_31[0][0]',         \n",
            "                                                                  'conv2d_35[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_35 (BatchN  (None, 8, 8, 180)   720         ['concatenate_32[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_35 (Activation)     (None, 8, 8, 180)    0           ['batch_normalization_35[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_36 (Conv2D)             (None, 8, 8, 18)     29160       ['activation_35[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_33 (Concatenate)   (None, 8, 8, 198)    0           ['concatenate_32[0][0]',         \n",
            "                                                                  'conv2d_36[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_36 (BatchN  (None, 8, 8, 198)   792         ['concatenate_33[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_36 (Activation)     (None, 8, 8, 198)    0           ['batch_normalization_36[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_37 (Conv2D)             (None, 8, 8, 18)     32076       ['activation_36[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_34 (Concatenate)   (None, 8, 8, 216)    0           ['concatenate_33[0][0]',         \n",
            "                                                                  'conv2d_37[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_37 (BatchN  (None, 8, 8, 216)   864         ['concatenate_34[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_37 (Activation)     (None, 8, 8, 216)    0           ['batch_normalization_37[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_38 (Conv2D)             (None, 8, 8, 18)     34992       ['activation_37[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_35 (Concatenate)   (None, 8, 8, 234)    0           ['concatenate_34[0][0]',         \n",
            "                                                                  'conv2d_38[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_38 (BatchN  (None, 8, 8, 234)   936         ['concatenate_35[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_38 (Activation)     (None, 8, 8, 234)    0           ['batch_normalization_38[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_39 (Conv2D)             (None, 8, 8, 18)     4212        ['activation_38[0][0]']          \n",
            "                                                                                                  \n",
            " average_pooling2d_2 (AveragePo  (None, 4, 4, 18)    0           ['conv2d_39[0][0]']              \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " batch_normalization_39 (BatchN  (None, 4, 4, 18)    72          ['average_pooling2d_2[0][0]']    \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_39 (Activation)     (None, 4, 4, 18)     0           ['batch_normalization_39[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_40 (Conv2D)             (None, 4, 4, 18)     2916        ['activation_39[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_36 (Concatenate)   (None, 4, 4, 36)     0           ['average_pooling2d_2[0][0]',    \n",
            "                                                                  'conv2d_40[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_40 (BatchN  (None, 4, 4, 36)    144         ['concatenate_36[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_40 (Activation)     (None, 4, 4, 36)     0           ['batch_normalization_40[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_41 (Conv2D)             (None, 4, 4, 18)     5832        ['activation_40[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_37 (Concatenate)   (None, 4, 4, 54)     0           ['concatenate_36[0][0]',         \n",
            "                                                                  'conv2d_41[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_41 (BatchN  (None, 4, 4, 54)    216         ['concatenate_37[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_41 (Activation)     (None, 4, 4, 54)     0           ['batch_normalization_41[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_42 (Conv2D)             (None, 4, 4, 18)     8748        ['activation_41[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_38 (Concatenate)   (None, 4, 4, 72)     0           ['concatenate_37[0][0]',         \n",
            "                                                                  'conv2d_42[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_42 (BatchN  (None, 4, 4, 72)    288         ['concatenate_38[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_42 (Activation)     (None, 4, 4, 72)     0           ['batch_normalization_42[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_43 (Conv2D)             (None, 4, 4, 18)     11664       ['activation_42[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_39 (Concatenate)   (None, 4, 4, 90)     0           ['concatenate_38[0][0]',         \n",
            "                                                                  'conv2d_43[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_43 (BatchN  (None, 4, 4, 90)    360         ['concatenate_39[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_43 (Activation)     (None, 4, 4, 90)     0           ['batch_normalization_43[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_44 (Conv2D)             (None, 4, 4, 18)     14580       ['activation_43[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_40 (Concatenate)   (None, 4, 4, 108)    0           ['concatenate_39[0][0]',         \n",
            "                                                                  'conv2d_44[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_44 (BatchN  (None, 4, 4, 108)   432         ['concatenate_40[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_44 (Activation)     (None, 4, 4, 108)    0           ['batch_normalization_44[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_45 (Conv2D)             (None, 4, 4, 18)     17496       ['activation_44[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_41 (Concatenate)   (None, 4, 4, 126)    0           ['concatenate_40[0][0]',         \n",
            "                                                                  'conv2d_45[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_45 (BatchN  (None, 4, 4, 126)   504         ['concatenate_41[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_45 (Activation)     (None, 4, 4, 126)    0           ['batch_normalization_45[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_46 (Conv2D)             (None, 4, 4, 18)     20412       ['activation_45[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_42 (Concatenate)   (None, 4, 4, 144)    0           ['concatenate_41[0][0]',         \n",
            "                                                                  'conv2d_46[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_46 (BatchN  (None, 4, 4, 144)   576         ['concatenate_42[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_46 (Activation)     (None, 4, 4, 144)    0           ['batch_normalization_46[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_47 (Conv2D)             (None, 4, 4, 18)     23328       ['activation_46[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_43 (Concatenate)   (None, 4, 4, 162)    0           ['concatenate_42[0][0]',         \n",
            "                                                                  'conv2d_47[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_47 (BatchN  (None, 4, 4, 162)   648         ['concatenate_43[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_47 (Activation)     (None, 4, 4, 162)    0           ['batch_normalization_47[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_48 (Conv2D)             (None, 4, 4, 18)     26244       ['activation_47[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_44 (Concatenate)   (None, 4, 4, 180)    0           ['concatenate_43[0][0]',         \n",
            "                                                                  'conv2d_48[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_48 (BatchN  (None, 4, 4, 180)   720         ['concatenate_44[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_48 (Activation)     (None, 4, 4, 180)    0           ['batch_normalization_48[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_49 (Conv2D)             (None, 4, 4, 18)     29160       ['activation_48[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_45 (Concatenate)   (None, 4, 4, 198)    0           ['concatenate_44[0][0]',         \n",
            "                                                                  'conv2d_49[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_49 (BatchN  (None, 4, 4, 198)   792         ['concatenate_45[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_49 (Activation)     (None, 4, 4, 198)    0           ['batch_normalization_49[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_50 (Conv2D)             (None, 4, 4, 18)     32076       ['activation_49[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_46 (Concatenate)   (None, 4, 4, 216)    0           ['concatenate_45[0][0]',         \n",
            "                                                                  'conv2d_50[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_50 (BatchN  (None, 4, 4, 216)   864         ['concatenate_46[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_50 (Activation)     (None, 4, 4, 216)    0           ['batch_normalization_50[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_51 (Conv2D)             (None, 4, 4, 18)     34992       ['activation_50[0][0]']          \n",
            "                                                                                                  \n",
            " concatenate_47 (Concatenate)   (None, 4, 4, 234)    0           ['concatenate_46[0][0]',         \n",
            "                                                                  'conv2d_51[0][0]']              \n",
            "                                                                                                  \n",
            " batch_normalization_51 (BatchN  (None, 4, 4, 234)   936         ['concatenate_47[0][0]']         \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_51 (Activation)     (None, 4, 4, 234)    0           ['batch_normalization_51[0][0]'] \n",
            "                                                                                                  \n",
            " average_pooling2d_3 (AveragePo  (None, 2, 2, 234)   0           ['activation_51[0][0]']          \n",
            " oling2D)                                                                                         \n",
            "                                                                                                  \n",
            " conv2d_52 (Conv2D)             (None, 1, 1, 10)     9370        ['average_pooling2d_3[0][0]']    \n",
            "                                                                                                  \n",
            " activation_52 (Activation)     (None, 1, 1, 10)     0           ['conv2d_52[0][0]']              \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 10)           0           ['activation_52[0][0]']          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 995,230\n",
            "Trainable params: 981,658\n",
            "Non-trainable params: 13,572\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FvxzSiQelA7h"
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(X_train)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9eJShdTGlZQ0"
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "ckpt = ModelCheckpoint(filepath = 'model.hdf5', monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PBRe7M__y5BD"
      },
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer=SGD(0.01, momentum = 0.7),metrics=['accuracy'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bytAIIYMld_h",
        "outputId": "b2da7446-f748-4fcd-86c7-9f2bf422edc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [ckpt])\n",
        "model.save_weights('30epochs.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "390/390 [==============================] - 121s 260ms/step - loss: 5.6446 - accuracy: 0.3960 - val_loss: 2.7081 - val_accuracy: 0.2483\n",
            "Epoch 2/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.8941 - accuracy: 0.4644 - val_loss: 2.2132 - val_accuracy: 0.3664\n",
            "Epoch 3/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.6627 - accuracy: 0.5283 - val_loss: 2.0398 - val_accuracy: 0.4091\n",
            "Epoch 4/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.5333 - accuracy: 0.5653 - val_loss: 1.7086 - val_accuracy: 0.5257\n",
            "Epoch 5/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.4435 - accuracy: 0.5940 - val_loss: 1.5918 - val_accuracy: 0.5513\n",
            "Epoch 6/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.3706 - accuracy: 0.6207 - val_loss: 1.4612 - val_accuracy: 0.5976\n",
            "Epoch 7/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.3094 - accuracy: 0.6412 - val_loss: 1.8103 - val_accuracy: 0.5070\n",
            "Epoch 8/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.2610 - accuracy: 0.6584 - val_loss: 2.8730 - val_accuracy: 0.3835\n",
            "Epoch 9/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.2188 - accuracy: 0.6736 - val_loss: 1.5185 - val_accuracy: 0.5933\n",
            "Epoch 10/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.1890 - accuracy: 0.6848 - val_loss: 1.5396 - val_accuracy: 0.5977\n",
            "Epoch 11/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.1553 - accuracy: 0.6974 - val_loss: 1.3017 - val_accuracy: 0.6585\n",
            "Epoch 12/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.1229 - accuracy: 0.7104 - val_loss: 1.2859 - val_accuracy: 0.6618\n",
            "Epoch 13/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.1051 - accuracy: 0.7158 - val_loss: 1.2208 - val_accuracy: 0.6922\n",
            "Epoch 14/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.0794 - accuracy: 0.7238 - val_loss: 1.0955 - val_accuracy: 0.7258\n",
            "Epoch 15/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 1.0537 - accuracy: 0.7343 - val_loss: 1.2668 - val_accuracy: 0.6738\n",
            "Epoch 16/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.0320 - accuracy: 0.7406 - val_loss: 1.4667 - val_accuracy: 0.6451\n",
            "Epoch 17/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 1.0113 - accuracy: 0.7481 - val_loss: 1.2169 - val_accuracy: 0.6938\n",
            "Epoch 18/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.9910 - accuracy: 0.7557 - val_loss: 1.1115 - val_accuracy: 0.7263\n",
            "Epoch 19/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.9764 - accuracy: 0.7602 - val_loss: 1.0589 - val_accuracy: 0.7416\n",
            "Epoch 20/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.9668 - accuracy: 0.7633 - val_loss: 1.1883 - val_accuracy: 0.7139\n",
            "Epoch 21/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.9443 - accuracy: 0.7705 - val_loss: 0.9585 - val_accuracy: 0.7665\n",
            "Epoch 22/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.9326 - accuracy: 0.7756 - val_loss: 0.9850 - val_accuracy: 0.7671\n",
            "Epoch 23/30\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.9122 - accuracy: 0.7810 - val_loss: 1.1195 - val_accuracy: 0.7258\n",
            "Epoch 24/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8997 - accuracy: 0.7866 - val_loss: 1.2522 - val_accuracy: 0.7031\n",
            "Epoch 25/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8943 - accuracy: 0.7864 - val_loss: 1.3157 - val_accuracy: 0.6848\n",
            "Epoch 26/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8755 - accuracy: 0.7932 - val_loss: 0.9966 - val_accuracy: 0.7546\n",
            "Epoch 27/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8656 - accuracy: 0.7962 - val_loss: 0.8230 - val_accuracy: 0.8161\n",
            "Epoch 28/30\n",
            "390/390 [==============================] - 98s 252ms/step - loss: 0.8512 - accuracy: 0.8022 - val_loss: 1.3414 - val_accuracy: 0.6823\n",
            "Epoch 29/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8432 - accuracy: 0.8037 - val_loss: 0.9908 - val_accuracy: 0.7615\n",
            "Epoch 30/30\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8438 - accuracy: 0.8022 - val_loss: 0.9693 - val_accuracy: 0.7715\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "FnPBMukIy_Gv",
        "outputId": "7b9ca76c-672b-4366-f36b-2292da59cbb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [ckpt])\n",
        "model.save_weights('60epochs.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.8250 - accuracy: 0.8095\n",
            "Epoch 1: val_accuracy improved from -inf to 0.81060, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8250 - accuracy: 0.8095 - val_loss: 0.8335 - val_accuracy: 0.8106\n",
            "Epoch 2/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.8168 - accuracy: 0.8118\n",
            "Epoch 2: val_accuracy did not improve from 0.81060\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.8168 - accuracy: 0.8118 - val_loss: 0.9283 - val_accuracy: 0.7832\n",
            "Epoch 3/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.8019 - accuracy: 0.8168\n",
            "Epoch 3: val_accuracy did not improve from 0.81060\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.8019 - accuracy: 0.8168 - val_loss: 0.9795 - val_accuracy: 0.7702\n",
            "Epoch 4/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.8023 - accuracy: 0.8188\n",
            "Epoch 4: val_accuracy improved from 0.81060 to 0.82460, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.8023 - accuracy: 0.8188 - val_loss: 0.7930 - val_accuracy: 0.8246\n",
            "Epoch 5/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7948 - accuracy: 0.8208\n",
            "Epoch 5: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7948 - accuracy: 0.8208 - val_loss: 0.9745 - val_accuracy: 0.7666\n",
            "Epoch 6/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.8226\n",
            "Epoch 6: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7824 - accuracy: 0.8226 - val_loss: 0.9109 - val_accuracy: 0.7937\n",
            "Epoch 7/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7824 - accuracy: 0.8257\n",
            "Epoch 7: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7824 - accuracy: 0.8257 - val_loss: 1.0156 - val_accuracy: 0.7622\n",
            "Epoch 8/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7683 - accuracy: 0.8286\n",
            "Epoch 8: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7683 - accuracy: 0.8286 - val_loss: 0.9246 - val_accuracy: 0.7836\n",
            "Epoch 9/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7582 - accuracy: 0.8313\n",
            "Epoch 9: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7582 - accuracy: 0.8313 - val_loss: 0.9513 - val_accuracy: 0.7836\n",
            "Epoch 10/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7599 - accuracy: 0.8297\n",
            "Epoch 10: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7599 - accuracy: 0.8297 - val_loss: 0.8725 - val_accuracy: 0.8016\n",
            "Epoch 11/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7427 - accuracy: 0.8353\n",
            "Epoch 11: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7427 - accuracy: 0.8353 - val_loss: 0.9498 - val_accuracy: 0.7812\n",
            "Epoch 12/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7436 - accuracy: 0.8362\n",
            "Epoch 12: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7436 - accuracy: 0.8362 - val_loss: 0.8276 - val_accuracy: 0.8115\n",
            "Epoch 13/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7348 - accuracy: 0.8389\n",
            "Epoch 13: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7348 - accuracy: 0.8389 - val_loss: 0.8172 - val_accuracy: 0.8104\n",
            "Epoch 14/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7225 - accuracy: 0.8413\n",
            "Epoch 14: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7225 - accuracy: 0.8413 - val_loss: 1.2689 - val_accuracy: 0.7225\n",
            "Epoch 15/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7205 - accuracy: 0.8414\n",
            "Epoch 15: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7205 - accuracy: 0.8414 - val_loss: 0.9719 - val_accuracy: 0.7834\n",
            "Epoch 16/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7166 - accuracy: 0.8441\n",
            "Epoch 16: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7166 - accuracy: 0.8441 - val_loss: 0.7830 - val_accuracy: 0.8233\n",
            "Epoch 17/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7108 - accuracy: 0.8442\n",
            "Epoch 17: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7108 - accuracy: 0.8442 - val_loss: 0.9724 - val_accuracy: 0.7765\n",
            "Epoch 18/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7117 - accuracy: 0.8454\n",
            "Epoch 18: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7117 - accuracy: 0.8454 - val_loss: 0.8753 - val_accuracy: 0.8027\n",
            "Epoch 19/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.7029 - accuracy: 0.8467\n",
            "Epoch 19: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.7029 - accuracy: 0.8467 - val_loss: 0.8632 - val_accuracy: 0.8090\n",
            "Epoch 20/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6902 - accuracy: 0.8498\n",
            "Epoch 20: val_accuracy did not improve from 0.82460\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6902 - accuracy: 0.8498 - val_loss: 0.8074 - val_accuracy: 0.8236\n",
            "Epoch 21/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.8514\n",
            "Epoch 21: val_accuracy improved from 0.82460 to 0.83360, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6867 - accuracy: 0.8514 - val_loss: 0.7528 - val_accuracy: 0.8336\n",
            "Epoch 22/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6863 - accuracy: 0.8506\n",
            "Epoch 22: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6863 - accuracy: 0.8506 - val_loss: 1.0782 - val_accuracy: 0.7472\n",
            "Epoch 23/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6792 - accuracy: 0.8545\n",
            "Epoch 23: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6792 - accuracy: 0.8545 - val_loss: 0.8249 - val_accuracy: 0.8164\n",
            "Epoch 24/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6784 - accuracy: 0.8539\n",
            "Epoch 24: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6784 - accuracy: 0.8539 - val_loss: 0.7832 - val_accuracy: 0.8296\n",
            "Epoch 25/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6672 - accuracy: 0.8585\n",
            "Epoch 25: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6672 - accuracy: 0.8585 - val_loss: 0.8583 - val_accuracy: 0.8104\n",
            "Epoch 26/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.8600\n",
            "Epoch 26: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6637 - accuracy: 0.8600 - val_loss: 0.8181 - val_accuracy: 0.8178\n",
            "Epoch 27/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6597 - accuracy: 0.8624\n",
            "Epoch 27: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6597 - accuracy: 0.8624 - val_loss: 1.0423 - val_accuracy: 0.7567\n",
            "Epoch 28/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6537 - accuracy: 0.8614\n",
            "Epoch 28: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6537 - accuracy: 0.8614 - val_loss: 0.8162 - val_accuracy: 0.8140\n",
            "Epoch 29/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6567 - accuracy: 0.8624\n",
            "Epoch 29: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6567 - accuracy: 0.8624 - val_loss: 0.7984 - val_accuracy: 0.8264\n",
            "Epoch 30/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.8642\n",
            "Epoch 30: val_accuracy did not improve from 0.83360\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6477 - accuracy: 0.8642 - val_loss: 0.8115 - val_accuracy: 0.8233\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "nRW7eK6F155B",
        "outputId": "9d131912-a4bc-4f6d-aade-aed7a996635e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('model.hdf5')\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(X_train)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [ckpt])\n",
        "model.save_weights('72epochs.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.8527\n",
            "Epoch 1: val_accuracy improved from 0.83360 to 0.84560, saving model to model.hdf5\n",
            "390/390 [==============================] - 99s 247ms/step - loss: 0.6849 - accuracy: 0.8527 - val_loss: 0.7226 - val_accuracy: 0.8456\n",
            "Epoch 2/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6782 - accuracy: 0.8548\n",
            "Epoch 2: val_accuracy did not improve from 0.84560\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6782 - accuracy: 0.8548 - val_loss: 0.7559 - val_accuracy: 0.8342\n",
            "Epoch 3/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.8552\n",
            "Epoch 3: val_accuracy did not improve from 0.84560\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6765 - accuracy: 0.8552 - val_loss: 0.7650 - val_accuracy: 0.8318\n",
            "Epoch 4/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6662 - accuracy: 0.8589\n",
            "Epoch 4: val_accuracy did not improve from 0.84560\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6662 - accuracy: 0.8589 - val_loss: 0.7908 - val_accuracy: 0.8210\n",
            "Epoch 5/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6628 - accuracy: 0.8589\n",
            "Epoch 5: val_accuracy did not improve from 0.84560\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6628 - accuracy: 0.8589 - val_loss: 0.7543 - val_accuracy: 0.8326\n",
            "Epoch 6/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6636 - accuracy: 0.8603\n",
            "Epoch 6: val_accuracy improved from 0.84560 to 0.85660, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6636 - accuracy: 0.8603 - val_loss: 0.6893 - val_accuracy: 0.8566\n",
            "Epoch 7/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6521 - accuracy: 0.8625\n",
            "Epoch 7: val_accuracy did not improve from 0.85660\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6521 - accuracy: 0.8625 - val_loss: 0.8352 - val_accuracy: 0.8110\n",
            "Epoch 8/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6543 - accuracy: 0.8633\n",
            "Epoch 8: val_accuracy improved from 0.85660 to 0.85840, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.6543 - accuracy: 0.8633 - val_loss: 0.6739 - val_accuracy: 0.8584\n",
            "Epoch 9/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6526 - accuracy: 0.8617\n",
            "Epoch 9: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.6526 - accuracy: 0.8617 - val_loss: 0.7662 - val_accuracy: 0.8285\n",
            "Epoch 10/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6455 - accuracy: 0.8663\n",
            "Epoch 10: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6455 - accuracy: 0.8663 - val_loss: 0.9171 - val_accuracy: 0.7995\n",
            "Epoch 11/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6448 - accuracy: 0.8659\n",
            "Epoch 11: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6448 - accuracy: 0.8659 - val_loss: 0.7495 - val_accuracy: 0.8426\n",
            "Epoch 12/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6392 - accuracy: 0.8674\n",
            "Epoch 12: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6392 - accuracy: 0.8674 - val_loss: 0.7234 - val_accuracy: 0.8422\n",
            "Epoch 13/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6361 - accuracy: 0.8688\n",
            "Epoch 13: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6361 - accuracy: 0.8688 - val_loss: 0.7730 - val_accuracy: 0.8285\n",
            "Epoch 14/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6279 - accuracy: 0.8685\n",
            "Epoch 14: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6279 - accuracy: 0.8685 - val_loss: 0.8952 - val_accuracy: 0.7994\n",
            "Epoch 15/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6229 - accuracy: 0.8725\n",
            "Epoch 15: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6229 - accuracy: 0.8725 - val_loss: 1.0680 - val_accuracy: 0.7632\n",
            "Epoch 16/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.8728\n",
            "Epoch 16: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6168 - accuracy: 0.8728 - val_loss: 0.8717 - val_accuracy: 0.8013\n",
            "Epoch 17/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6162 - accuracy: 0.8723\n",
            "Epoch 17: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6162 - accuracy: 0.8723 - val_loss: 0.7510 - val_accuracy: 0.8374\n",
            "Epoch 18/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6150 - accuracy: 0.8746\n",
            "Epoch 18: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6150 - accuracy: 0.8746 - val_loss: 0.8873 - val_accuracy: 0.8012\n",
            "Epoch 19/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6126 - accuracy: 0.8749\n",
            "Epoch 19: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6126 - accuracy: 0.8749 - val_loss: 0.8035 - val_accuracy: 0.8198\n",
            "Epoch 20/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6135 - accuracy: 0.8749\n",
            "Epoch 20: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6135 - accuracy: 0.8749 - val_loss: 0.7702 - val_accuracy: 0.8315\n",
            "Epoch 21/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.6010 - accuracy: 0.8777\n",
            "Epoch 21: val_accuracy did not improve from 0.85840\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.6010 - accuracy: 0.8777 - val_loss: 0.9920 - val_accuracy: 0.7805\n",
            "Epoch 22/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5970 - accuracy: 0.8773\n",
            "Epoch 22: val_accuracy improved from 0.85840 to 0.86310, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.5970 - accuracy: 0.8773 - val_loss: 0.6800 - val_accuracy: 0.8631\n",
            "Epoch 23/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5967 - accuracy: 0.8791\n",
            "Epoch 23: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.5967 - accuracy: 0.8791 - val_loss: 0.7156 - val_accuracy: 0.8448\n",
            "Epoch 24/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5949 - accuracy: 0.8789\n",
            "Epoch 24: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.5949 - accuracy: 0.8789 - val_loss: 0.7696 - val_accuracy: 0.8320\n",
            "Epoch 25/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5941 - accuracy: 0.8778\n",
            "Epoch 25: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.5941 - accuracy: 0.8778 - val_loss: 0.7249 - val_accuracy: 0.8460\n",
            "Epoch 26/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5861 - accuracy: 0.8816\n",
            "Epoch 26: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.5861 - accuracy: 0.8816 - val_loss: 0.9513 - val_accuracy: 0.7928\n",
            "Epoch 27/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5936 - accuracy: 0.8812\n",
            "Epoch 27: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.5936 - accuracy: 0.8812 - val_loss: 0.8272 - val_accuracy: 0.8217\n",
            "Epoch 28/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5843 - accuracy: 0.8829\n",
            "Epoch 28: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.5843 - accuracy: 0.8829 - val_loss: 0.7373 - val_accuracy: 0.8442\n",
            "Epoch 29/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5772 - accuracy: 0.8851\n",
            "Epoch 29: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.5772 - accuracy: 0.8851 - val_loss: 0.7720 - val_accuracy: 0.8369\n",
            "Epoch 30/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.5797 - accuracy: 0.8846\n",
            "Epoch 30: val_accuracy did not improve from 0.86310\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.5797 - accuracy: 0.8846 - val_loss: 0.6840 - val_accuracy: 0.8576\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "KyrYDxouDFyW",
        "outputId": "761e881e-a8b8-410b-fb75-65b0a49153ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('72epochs.h5')\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(X_train)\n",
        "keras.backend.set_value(model.optimizer.lr, .001)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 30, validation_data =(X_test, y_test), callbacks = [ckpt])\n",
        "model.save_weights('102epochs.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.9088\n",
            "Epoch 1: val_accuracy improved from 0.86310 to 0.88320, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.3832 - accuracy: 0.9088 - val_loss: 0.4520 - val_accuracy: 0.8832\n",
            "Epoch 2/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3392 - accuracy: 0.9147\n",
            "Epoch 2: val_accuracy did not improve from 0.88320\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3392 - accuracy: 0.9147 - val_loss: 0.4460 - val_accuracy: 0.8830\n",
            "Epoch 3/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9182\n",
            "Epoch 3: val_accuracy improved from 0.88320 to 0.89380, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.3346 - accuracy: 0.9182 - val_loss: 0.4114 - val_accuracy: 0.8938\n",
            "Epoch 4/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.9175\n",
            "Epoch 4: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3298 - accuracy: 0.9175 - val_loss: 0.4174 - val_accuracy: 0.8898\n",
            "Epoch 5/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3259 - accuracy: 0.9186\n",
            "Epoch 5: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3259 - accuracy: 0.9186 - val_loss: 0.4133 - val_accuracy: 0.8907\n",
            "Epoch 6/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3317 - accuracy: 0.9161\n",
            "Epoch 6: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3317 - accuracy: 0.9161 - val_loss: 0.4188 - val_accuracy: 0.8911\n",
            "Epoch 7/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3333 - accuracy: 0.9156\n",
            "Epoch 7: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3333 - accuracy: 0.9156 - val_loss: 0.4674 - val_accuracy: 0.8790\n",
            "Epoch 8/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3307 - accuracy: 0.9170\n",
            "Epoch 8: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3307 - accuracy: 0.9170 - val_loss: 0.4461 - val_accuracy: 0.8815\n",
            "Epoch 9/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9179\n",
            "Epoch 9: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3319 - accuracy: 0.9179 - val_loss: 0.4364 - val_accuracy: 0.8882\n",
            "Epoch 10/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9165\n",
            "Epoch 10: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3319 - accuracy: 0.9165 - val_loss: 0.4956 - val_accuracy: 0.8750\n",
            "Epoch 11/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.9146\n",
            "Epoch 11: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3381 - accuracy: 0.9146 - val_loss: 0.5056 - val_accuracy: 0.8705\n",
            "Epoch 12/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3381 - accuracy: 0.9153\n",
            "Epoch 12: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3381 - accuracy: 0.9153 - val_loss: 0.4844 - val_accuracy: 0.8789\n",
            "Epoch 13/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3357 - accuracy: 0.9156\n",
            "Epoch 13: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3357 - accuracy: 0.9156 - val_loss: 0.4867 - val_accuracy: 0.8741\n",
            "Epoch 14/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9163\n",
            "Epoch 14: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3346 - accuracy: 0.9163 - val_loss: 0.4831 - val_accuracy: 0.8774\n",
            "Epoch 15/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3368 - accuracy: 0.9162\n",
            "Epoch 15: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3368 - accuracy: 0.9162 - val_loss: 0.4578 - val_accuracy: 0.8825\n",
            "Epoch 16/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3359 - accuracy: 0.9165\n",
            "Epoch 16: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3359 - accuracy: 0.9165 - val_loss: 0.4705 - val_accuracy: 0.8796\n",
            "Epoch 17/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3370 - accuracy: 0.9159\n",
            "Epoch 17: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3370 - accuracy: 0.9159 - val_loss: 0.4967 - val_accuracy: 0.8737\n",
            "Epoch 18/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3319 - accuracy: 0.9168\n",
            "Epoch 18: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3319 - accuracy: 0.9168 - val_loss: 0.4686 - val_accuracy: 0.8831\n",
            "Epoch 19/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3378 - accuracy: 0.9164\n",
            "Epoch 19: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3378 - accuracy: 0.9164 - val_loss: 0.5128 - val_accuracy: 0.8710\n",
            "Epoch 20/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3324 - accuracy: 0.9175\n",
            "Epoch 20: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3324 - accuracy: 0.9175 - val_loss: 0.4517 - val_accuracy: 0.8842\n",
            "Epoch 21/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3326 - accuracy: 0.9173\n",
            "Epoch 21: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3326 - accuracy: 0.9173 - val_loss: 0.4721 - val_accuracy: 0.8802\n",
            "Epoch 22/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3308 - accuracy: 0.9189\n",
            "Epoch 22: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3308 - accuracy: 0.9189 - val_loss: 0.5142 - val_accuracy: 0.8718\n",
            "Epoch 23/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3313 - accuracy: 0.9190\n",
            "Epoch 23: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3313 - accuracy: 0.9190 - val_loss: 0.4988 - val_accuracy: 0.8755\n",
            "Epoch 24/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3294 - accuracy: 0.9185\n",
            "Epoch 24: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3294 - accuracy: 0.9185 - val_loss: 0.5332 - val_accuracy: 0.8644\n",
            "Epoch 25/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3283 - accuracy: 0.9188\n",
            "Epoch 25: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3283 - accuracy: 0.9188 - val_loss: 0.5118 - val_accuracy: 0.8693\n",
            "Epoch 26/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3312 - accuracy: 0.9185\n",
            "Epoch 26: val_accuracy did not improve from 0.89380\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3312 - accuracy: 0.9185 - val_loss: 0.4915 - val_accuracy: 0.8730\n",
            "Epoch 27/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3309 - accuracy: 0.9180\n",
            "Epoch 27: val_accuracy improved from 0.89380 to 0.89520, saving model to model.hdf5\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.3309 - accuracy: 0.9180 - val_loss: 0.4196 - val_accuracy: 0.8952\n",
            "Epoch 28/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3270 - accuracy: 0.9195\n",
            "Epoch 28: val_accuracy did not improve from 0.89520\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.3270 - accuracy: 0.9195 - val_loss: 0.4580 - val_accuracy: 0.8836\n",
            "Epoch 29/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3316 - accuracy: 0.9178\n",
            "Epoch 29: val_accuracy did not improve from 0.89520\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3316 - accuracy: 0.9178 - val_loss: 0.5085 - val_accuracy: 0.8707\n",
            "Epoch 30/30\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3305 - accuracy: 0.9202\n",
            "Epoch 30: val_accuracy did not improve from 0.89520\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3305 - accuracy: 0.9202 - val_loss: 0.5296 - val_accuracy: 0.8698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('102epochs.h5')\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "datagen = ImageDataGenerator(rotation_range = 15, horizontal_flip = True, width_shift_range = 0.1, height_shift_range = 0.1, zoom_range = 0.2, shear_range = 15)\n",
        "datagen.fit(X_train)\n",
        "keras.backend.set_value(model.optimizer.lr, .001)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 10, validation_data =(X_test, y_test), callbacks = [ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-tCtw6j2qrv",
        "outputId": "717606bb-acd1-4fe1-e32f-f6747c8e31d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3303 - accuracy: 0.9191\n",
            "Epoch 1: val_accuracy improved from -inf to 0.87360, saving model to model.hdf5\n",
            "390/390 [==============================] - 122s 262ms/step - loss: 0.3303 - accuracy: 0.9191 - val_loss: 0.5085 - val_accuracy: 0.8736\n",
            "Epoch 2/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.9195\n",
            "Epoch 2: val_accuracy improved from 0.87360 to 0.88990, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.3286 - accuracy: 0.9195 - val_loss: 0.4439 - val_accuracy: 0.8899\n",
            "Epoch 3/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9206\n",
            "Epoch 3: val_accuracy did not improve from 0.88990\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.3282 - accuracy: 0.9206 - val_loss: 0.4681 - val_accuracy: 0.8822\n",
            "Epoch 4/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3204 - accuracy: 0.9211\n",
            "Epoch 4: val_accuracy did not improve from 0.88990\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.3204 - accuracy: 0.9211 - val_loss: 0.5363 - val_accuracy: 0.8673\n",
            "Epoch 5/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.9201\n",
            "Epoch 5: val_accuracy did not improve from 0.88990\n",
            "390/390 [==============================] - 98s 251ms/step - loss: 0.3297 - accuracy: 0.9201 - val_loss: 0.5123 - val_accuracy: 0.8730\n",
            "Epoch 6/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3282 - accuracy: 0.9192\n",
            "Epoch 6: val_accuracy did not improve from 0.88990\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.3282 - accuracy: 0.9192 - val_loss: 0.4475 - val_accuracy: 0.8892\n",
            "Epoch 7/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3226 - accuracy: 0.9216\n",
            "Epoch 7: val_accuracy improved from 0.88990 to 0.89010, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3226 - accuracy: 0.9216 - val_loss: 0.4393 - val_accuracy: 0.8901\n",
            "Epoch 8/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3260 - accuracy: 0.9191\n",
            "Epoch 8: val_accuracy did not improve from 0.89010\n",
            "390/390 [==============================] - 104s 265ms/step - loss: 0.3260 - accuracy: 0.9191 - val_loss: 0.4475 - val_accuracy: 0.8885\n",
            "Epoch 9/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3235 - accuracy: 0.9203\n",
            "Epoch 9: val_accuracy did not improve from 0.89010\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.3235 - accuracy: 0.9203 - val_loss: 0.5388 - val_accuracy: 0.8619\n",
            "Epoch 10/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.9193\n",
            "Epoch 10: val_accuracy improved from 0.89010 to 0.89120, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.3268 - accuracy: 0.9193 - val_loss: 0.4397 - val_accuracy: 0.8912\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f955e37c8d0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rotation_range = 5, horizontal_flip = True, width_shift_range = 0.05, height_shift_range = 0.05, shear_range = 5)\n",
        "datagen.fit(X_train)\n",
        "keras.backend.set_value(model.optimizer.lr, .001)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 10, validation_data =(X_test, y_test), callbacks = [ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTpRkb2r-Jv8",
        "outputId": "adcbfa14-a67a-4016-a76a-f09351520af8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2152 - accuracy: 0.9535\n",
            "Epoch 1: val_accuracy did not improve from 0.89120\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.2152 - accuracy: 0.9535 - val_loss: 0.4339 - val_accuracy: 0.8849\n",
            "Epoch 2/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2176 - accuracy: 0.9536\n",
            "Epoch 2: val_accuracy improved from 0.89120 to 0.89700, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.2176 - accuracy: 0.9536 - val_loss: 0.3996 - val_accuracy: 0.8970\n",
            "Epoch 3/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2201 - accuracy: 0.9523\n",
            "Epoch 3: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 96s 247ms/step - loss: 0.2201 - accuracy: 0.9523 - val_loss: 0.4434 - val_accuracy: 0.8856\n",
            "Epoch 4/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2222 - accuracy: 0.9528\n",
            "Epoch 4: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.2222 - accuracy: 0.9528 - val_loss: 0.4145 - val_accuracy: 0.8931\n",
            "Epoch 5/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2210 - accuracy: 0.9534\n",
            "Epoch 5: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 98s 250ms/step - loss: 0.2210 - accuracy: 0.9534 - val_loss: 0.4887 - val_accuracy: 0.8757\n",
            "Epoch 6/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2173 - accuracy: 0.9549\n",
            "Epoch 6: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.2173 - accuracy: 0.9549 - val_loss: 0.4889 - val_accuracy: 0.8752\n",
            "Epoch 7/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2179 - accuracy: 0.9528\n",
            "Epoch 7: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.2179 - accuracy: 0.9528 - val_loss: 0.4973 - val_accuracy: 0.8729\n",
            "Epoch 8/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2196 - accuracy: 0.9529\n",
            "Epoch 8: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.2196 - accuracy: 0.9529 - val_loss: 0.4462 - val_accuracy: 0.8871\n",
            "Epoch 9/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2182 - accuracy: 0.9536\n",
            "Epoch 9: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 96s 247ms/step - loss: 0.2182 - accuracy: 0.9536 - val_loss: 0.4564 - val_accuracy: 0.8842\n",
            "Epoch 10/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9554\n",
            "Epoch 10: val_accuracy did not improve from 0.89700\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.2155 - accuracy: 0.9554 - val_loss: 0.4247 - val_accuracy: 0.8933\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f955e32eb90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(rotation_range = 5, horizontal_flip = True, width_shift_range = 0.05, height_shift_range = 0.05, shear_range = 5)\n",
        "datagen.fit(X_train)\n",
        "keras.backend.set_value(model.optimizer.lr, .0001)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = X_train.shape[0]/batch_size, epochs = 10, validation_data =(X_test, y_test), callbacks = [ckpt])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdhJTf0XCXZ_",
        "outputId": "c49c5adf-ffd5-42de-92b3-5f38ca3ee564"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1701 - accuracy: 0.9608\n",
            "Epoch 1: val_accuracy improved from 0.89700 to 0.89750, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 249ms/step - loss: 0.1701 - accuracy: 0.9608 - val_loss: 0.3613 - val_accuracy: 0.8975\n",
            "Epoch 2/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9621\n",
            "Epoch 2: val_accuracy improved from 0.89750 to 0.90080, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1531 - accuracy: 0.9621 - val_loss: 0.3533 - val_accuracy: 0.9008\n",
            "Epoch 3/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1496 - accuracy: 0.9626\n",
            "Epoch 3: val_accuracy did not improve from 0.90080\n",
            "390/390 [==============================] - 96s 247ms/step - loss: 0.1496 - accuracy: 0.9626 - val_loss: 0.3548 - val_accuracy: 0.9005\n",
            "Epoch 4/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9628\n",
            "Epoch 4: val_accuracy improved from 0.90080 to 0.90130, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1477 - accuracy: 0.9628 - val_loss: 0.3483 - val_accuracy: 0.9013\n",
            "Epoch 5/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1501 - accuracy: 0.9618\n",
            "Epoch 5: val_accuracy did not improve from 0.90130\n",
            "390/390 [==============================] - 96s 247ms/step - loss: 0.1501 - accuracy: 0.9618 - val_loss: 0.3572 - val_accuracy: 0.8990\n",
            "Epoch 6/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1483 - accuracy: 0.9624\n",
            "Epoch 6: val_accuracy improved from 0.90130 to 0.90300, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1483 - accuracy: 0.9624 - val_loss: 0.3440 - val_accuracy: 0.9030\n",
            "Epoch 7/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1465 - accuracy: 0.9624\n",
            "Epoch 7: val_accuracy improved from 0.90300 to 0.90420, saving model to model.hdf5\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1465 - accuracy: 0.9624 - val_loss: 0.3402 - val_accuracy: 0.9042\n",
            "Epoch 8/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1498 - accuracy: 0.9621\n",
            "Epoch 8: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.1498 - accuracy: 0.9621 - val_loss: 0.3632 - val_accuracy: 0.8988\n",
            "Epoch 9/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9625\n",
            "Epoch 9: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.1493 - accuracy: 0.9625 - val_loss: 0.3542 - val_accuracy: 0.9010\n",
            "Epoch 10/10\n",
            "391/390 [==============================] - ETA: 0s - loss: 0.1463 - accuracy: 0.9623\n",
            "Epoch 10: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.1463 - accuracy: 0.9623 - val_loss: 0.3531 - val_accuracy: 0.8993\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f954494e9d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keras.backend.set_value(model.optimizer.lr, 0.0005)\n",
        "keras.backend.set_value(model.optimizer.momentum, 0.5)\n",
        "bacth_size = 512\n",
        "datagen = ImageDataGenerator(rotation_range = 5, horizontal_flip = True, width_shift_range = 0.05, height_shift_range = 0.05, shear_range = 5)\n",
        "datagen.fit(X_train)\n",
        "model.fit(datagen.flow(X_train, y_train, batch_size), steps_per_epoch = len(X_train)//batch_size, epochs = 5, validation_data =(X_test, y_test), callbacks = [ckpt])"
      ],
      "metadata": {
        "id": "__eoJIWj12zR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c339524-5de0-4fc0-ba82-fd8958dd2c32"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1651 - accuracy: 0.9592\n",
            "Epoch 1: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1651 - accuracy: 0.9592 - val_loss: 0.4021 - val_accuracy: 0.8909\n",
            "Epoch 2/5\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1680 - accuracy: 0.9599\n",
            "Epoch 2: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 248ms/step - loss: 0.1680 - accuracy: 0.9599 - val_loss: 0.4037 - val_accuracy: 0.8901\n",
            "Epoch 3/5\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1659 - accuracy: 0.9610\n",
            "Epoch 3: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.1659 - accuracy: 0.9610 - val_loss: 0.3762 - val_accuracy: 0.8974\n",
            "Epoch 4/5\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1683 - accuracy: 0.9590\n",
            "Epoch 4: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 97s 247ms/step - loss: 0.1683 - accuracy: 0.9590 - val_loss: 0.3860 - val_accuracy: 0.8947\n",
            "Epoch 5/5\n",
            "390/390 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9592\n",
            "Epoch 5: val_accuracy did not improve from 0.90420\n",
            "390/390 [==============================] - 96s 247ms/step - loss: 0.1694 - accuracy: 0.9592 - val_loss: 0.3816 - val_accuracy: 0.8962\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9544391b10>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9QFUEn64WPlt",
        "outputId": "245bb8df-9c1b-4c27-ac97-62e10e595028"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 6s 18ms/step - loss: 0.3523 - accuracy: 0.9009\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.35229936242103577, 0.9009000062942505]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The validation accuracy has been improved to 90."
      ],
      "metadata": {
        "id": "Gwb01_nrNzyv"
      }
    }
  ]
}